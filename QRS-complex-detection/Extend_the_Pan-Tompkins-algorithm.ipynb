{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8cff2a-dd21-4a9c-9c2a-acbdfbdd3dbe",
   "metadata": {},
   "source": [
    "**Exercise 1: Extending the Pan-Tompkins Algorithm**\n",
    "\n",
    "The Pan-Tompkins algorithm in the previous video is a basic version of the algorithm. In this exercise we will add features to the decision rules to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada5771-714e-4ec6-8aa6-bfb5b213c29c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc46cfd4-0cd1-4b8b-a172-2a7fa402f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ts = np.arange(0, 5, 1/100)\n",
    "sinusoid = 3 * np.sin(2 * np.pi * 1 * ts + np.pi) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a509a605-952a-4c51-b914-d2cf0cefbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa425-2ac4-4858-a4b7-00a1ae706813",
   "metadata": {},
   "source": [
    "**Performance Evaluation Helpers**\n",
    "\n",
    "First, we need to build a function that tells us the performance of our QRS estimates. We will optimize for precision and recall. These two functions should help us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50793522-0c84-4b55-9c10-dda6a1e5ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(reference_peak_indices, estimate_peak_indices, tolerance_samples=40):\n",
    "    \"\"\"Evaluates algorithm performance for a single dataset.\n",
    "    \n",
    "    It is not expected that reference and estimate peak indices overlap exactly.\n",
    "    Instead say a QRS estimate is correct if it is within <tolerance_samples> of\n",
    "    a reference estimate.\n",
    "    \n",
    "    Args:\n",
    "        reference_peak_indices: (np.array) ground-truth array of QRS complex locations\n",
    "        estiamte_peak_indices: (np.array) array of QRS complex estimates\n",
    "        tolerance_samples: (number) How close a QRS estimate needs to be to a reference\n",
    "            location to be correct.\n",
    "    Returns:\n",
    "        n_correct: (number) The number of QRS complexes that were correctly detected\n",
    "        n_missed: (number) The number of QRS complexes that the algorithm failed\n",
    "            to detect\n",
    "        n_extra: (number) The number of spurious QRS complexes detected by the\n",
    "            algorithm\n",
    "    \"\"\"\n",
    "    # Keep track of the number of QRS peaks that were found correctly\n",
    "    n_correct = 0\n",
    "    # ... that were missed\n",
    "    n_missed = 0\n",
    "    # ... and that are spurious\n",
    "    n_extra = 0\n",
    "    \n",
    "    # Loop counters\n",
    "    i, j = 0, 0\n",
    "    while (i < len(reference_peak_indices)) and (j < len(estimate_peak_indices)):\n",
    "        # Iterate through the arrays of QRS peaks, counting the number of peaks\n",
    "        # that are correct, missed, and extra.\n",
    "        ref = reference_peak_indices[i]\n",
    "        est = estimate_peak_indices[j]\n",
    "        if abs(ref - est) < tolerance_samples:\n",
    "            # If the reference peak and the estimate peak are within <tolerance_samples>,\n",
    "            # then we mark this beat correctly detected and move on to the next one.\n",
    "            n_correct += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "            continue\n",
    "        if ref < est:\n",
    "            # Else, if they are farther apart and the reference is before the estimate,\n",
    "            # then the detector missed a beat and we advance the reference array.\n",
    "            n_missed += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        # Else, the estimate is before the reference. This means we found an extra beat\n",
    "        # in the estimate array. We advance the estimate array to check the next beat.\n",
    "        j += 1\n",
    "        n_extra += 1\n",
    "    # Don't forget to count the number of missed or extra peaks at the end of the array.\n",
    "    n_missed += len(reference_peak_indices[i:])\n",
    "    n_extra += len(estimate_peak_indices[j:])\n",
    "    return n_correct, n_missed, n_extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64390e-6d85-4e70-b7b5-bebdae9455d8",
   "metadata": {},
   "source": [
    "Now we need a function that can compute precision and recall for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb5c1b2-7040-4bf9-a7f0-f96a71f1f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrecisionRecall(n_correct, n_missed, n_extra):\n",
    "    # TODO: Compute precision and recall from the input arguments.\n",
    "    precision = n_correct / (n_correct + n_extra)\n",
    "    recall = n_correct / (n_correct + n_missed)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d648a1-8b9b-4f4d-8775-bf2e29c8a667",
   "metadata": {},
   "source": [
    "## Pan-Tompkins Algorithm\n",
    "\n",
    "We will start with the same algorithm that you saw in the last video. This starter code differs only in that we do not *LocalizeMaxima* on the output peaks. This is because for this dataset the QRS complexes could be pointing up or down and if we try to find the maxima when the QRS complex is pointing downward we will hurt our algorithm performance. Instead we will be happy with the approximate QRS locations that our algorithm detects.\n",
    "\n",
    "The current version of the algorithm has a precision and recall of 0.89 and 0.74. Verify this by running the next cell. Your task is to improve the performance of the algorithm by adding the following features.\n",
    "\n",
    "### Refractory Period Blanking\n",
    "Recall from the physiology lesson that the QRS complex is a result of ventricular depolarization, and that cellular depolarization happens when ions travel across the cell membrane. There is a physiological constraint on how soon consecutive depolarization can occur. This constraint is 200 ms. Read more about it [here](https://en.wikipedia.org/wiki/Refractory_period_(physiology)#Cardiac_refractory_period). We can take advantage of this phenomenon in our algorithm by removing detections that occur within 200ms of another one. Preserve the larger detection.\n",
    "\n",
    "### Adaptive Thresholding\n",
    "The QRS complex height can change over time as contact with the electrodes changes or shifts. Instead of using a fixed threshold, we should use one that changes over time. Make the detection threshold 70% of the average peak height for the last 8 peaks.\n",
    "\n",
    "### T-Wave Discrimination\n",
    "One error mode is to detect T-waves as QRS complexes. We can avoid picking T-waves by doing the following:\n",
    "  * Find peaks that follow a previous one by 360ms or less\n",
    "  * Compute the maximum absolute slope within 60ms of each peak. Eg `np.max(np.abs(np.diff(ecg[peak - 60ms: peak + 60ms])))`\n",
    "  * If the slope of the second peak is less than half of the slope of the first peak, discard the second peak as a T-wave\n",
    "Read another description of this technique [here](https://en.wikipedia.org/wiki/Pan%E2%80%93Tompkins_algorithm#T_wave_discrimination)\n",
    "\n",
    "After implementing these three techniques you should see a significant increase in precision and recall. I ended up with 0.95 and 0.87. See if you can beat that! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc02f83-c149-4f69-848b-4763665316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BandpassFilter(signal, fs=300):\n",
    "    \"\"\"Bandpass filter the signal between 5 and 15 Hz.\"\"\"\n",
    "    b, a = sp.signal.butter(3, (5, 15), btype='bandpass', fs=fs)\n",
    "    return sp.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def MovingSum(signal, fs=300):\n",
    "    \"\"\"Moving sum operation with window size of 150ms.\"\"\"\n",
    "    n_samples = int(round(fs * 0.150))\n",
    "    return pd.Series(signal).rolling(n_samples, center=True).sum().values\n",
    "\n",
    "def FindPeaks(signal, order=10):\n",
    "    \"\"\"A simple peak detection algorithm.\"\"\"\n",
    "    msk = (signal[order:-order] > signal[:-order * 2]) & (signal[order:-order] > signal[order * 2:])\n",
    "    for o in range(1, order):\n",
    "        msk &= (signal[order:-order] > signal[o: -order * 2 + o])\n",
    "        msk &= (signal[order:-order] > signal[order * 2 - o: -o])\n",
    "    return msk.nonzero()[0] + order\n",
    "\n",
    "def ThresholdPeaks(filtered_signal, peaks):\n",
    "    \"\"\"Threshold detected peaks to select the QRS complexes.\"\"\"\n",
    "    thresh = np.mean(filtered_signal[peaks])\n",
    "    return peaks[filtered_signal[peaks] > thresh]\n",
    "\n",
    "def AdaptiveThresholdPeaks(filtered_signal, peaks):\n",
    "    \"\"\"Adaptive thresholding\"\"\"\n",
    "    thresh = pd.Series(filtered_signal[peaks]).rolling(8).mean() * .7\n",
    "    return peaks[filtered_signal[peaks] > thresh]\n",
    "\n",
    "    \n",
    "def RefractoryPeriodBlanking(filtered_signal, peaks, fs, refractory_period_ms=200):\n",
    "    \"\"\"Refractory period blanking\"\"\"\n",
    "    # First sort peaks by height\n",
    "    peaks_by_height = sorted(zip(filtered_signal[peaks], peaks), reverse=True)\n",
    "    \n",
    "    # Compute the refractory period in samples\n",
    "    ref_pd = int(refractory_period_ms * fs / 1000)\n",
    "    \n",
    "    # Initialize our new_peaks list with the largest peak\n",
    "    new_peaks = [peaks_by_height[0][1]]\n",
    "    \n",
    "    # For each subsequent peak, if it does not violet the refractory period of any other peak in new peaks,\n",
    "    #add it to new_peaks\n",
    "    for _, peak in peaks_by_height:\n",
    "        if np.all(np.abs(np.subtract(new_peaks, peak)) > ref_pd):\n",
    "            new_peaks.append(peak)\n",
    "    return np.array(sorted(new_peaks))\n",
    "\n",
    "def TWaveDiscrimination(signal, peaks, fs, twave_pd_ms=360, slope_window_ms=60):\n",
    "    \"\"\"Compute t-wave discrimination\"\"\" \n",
    "    # Compute the T-wave period in samples\n",
    "    twave_pd = twave_pd_ms * fs / 1000\n",
    "    \n",
    "    # compute the slope window in samples\n",
    "    slope_win = int(slope_window_ms * fs / 1000)\n",
    "    \n",
    "    #Find all possible peaks within the twave_pd_ms of a preceeding peak and mark it\n",
    "    # as possible T-wave\n",
    "    possible_twave_inds = ((peaks[1:] - peaks[:-1]) < twave_pd).nonzero()[0]\n",
    "    twave_inds = []\n",
    "    \n",
    "    # For each possible T-wave...\n",
    "    for ind in possible_twave_inds:\n",
    "        pk = peaks[ind]\n",
    "        \n",
    "        #Find the maximum slope around this wave\n",
    "        slope = np.max(np.abs(np.diff(signal[pk - slope_win: pk + slope_win])))\n",
    "        \n",
    "        ## And find the maximum slope around the preceeding wave\n",
    "        prev_pk = peaks[ind - 1]\n",
    "        prev_slope = np.max(np.abs(np.diff(\n",
    "            signal[prev_pk - slope_win: prev_pk + slope_win])))\n",
    "        \n",
    "        # If this slope is less than half the preceeding slope...\n",
    "        if prev_slope > slope * 2:\n",
    "            # ... this is a T-wave\n",
    "            twave_inds.append(ind)\n",
    "    return np.delete(peaks, twave_inds)\n",
    "\n",
    "def PanTompkinsPeaks(signal, fs):\n",
    "    \"\"\"Pan-Tompkins QRS complex detection algorithm.\"\"\"\n",
    "    filtered_signal = MovingSum(\n",
    "        np.square(\n",
    "            np.diff(\n",
    "                BandpassFilter(signal, fs))), fs)\n",
    "    peaks = FindPeaks(filtered_signal)\n",
    "    peaks = RefractoryPeriodBlanking(filtered_signal, peaks, fs)  # TODO: Uncomment this line\n",
    "    #peaks = ThresholdPeaks(filtered_signal, peaks)                 # TODO: Remove this line\n",
    "    peaks = AdaptiveThresholdPeaks(filtered_signal, peaks)        # TODO: Uncomment this line\n",
    "    peaks = TWaveDiscrimination(signal, peaks, fs)                # TODO: Uncomment this line\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f213dc5-e0d1-484c-8248-a1f4207724fc",
   "metadata": {},
   "source": [
    "## Load Data and Evaluate Performance\n",
    "\n",
    "As we add features to the algorithm we can continue to evaluate it and see the change in performance.  Use the code below to compute an overall precision and recall for QRS detection. You must first implement the `PrecisionRecall` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea80785-c4e9-4d24-ac1f-a68bf5b21d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/2 files processed...\r",
      "2/2 files processed...\n",
      "Total performance:\n",
      "\tPrecision = 0.88\n",
      "\tRecall = 0.96\n"
     ]
    }
   ],
   "source": [
    "# This dataset is sampled at 300 Hz.\n",
    "fs = 300\n",
    "files = glob.glob('data/*.npz')\n",
    "\n",
    "# Keep track of the total number of correct, missed, and extra detections.\n",
    "total_correct, total_missed, total_extra = 0, 0, 0\n",
    "\n",
    "for i, fl in enumerate(files):\n",
    "    # For each file, load the data...\n",
    "    with np.load(fl) as npz:\n",
    "        ecg = npz['ecg']\n",
    "        reference_peak_indices = npz['qrs']\n",
    "    # Compute our QRS location estimates...\n",
    "    estimate_peak_indices = PanTompkinsPeaks(ecg, fs)\n",
    "\n",
    "    # Compare our estimates against the reference...\n",
    "    n_correct, n_missed, n_extra = Evaluate(reference_peak_indices, estimate_peak_indices)\n",
    "\n",
    "    # And add them to our running totals.\n",
    "    total_correct += n_correct\n",
    "    total_missed += n_missed\n",
    "    total_extra += n_extra\n",
    "    print('\\r{}/{} files processed...'.format(i+1, len(files)), end='')\n",
    "print('') # print a newline\n",
    "\n",
    "# Compute and report the overall performance.\n",
    "precision, recall = PrecisionRecall(total_correct, total_missed, total_extra)\n",
    "print('Total performance:\\n\\tPrecision = {:0.2f}\\n\\tRecall = {:0.2f}'.format(precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458f80f-7479-44a6-bd15-56ab56577b44",
   "metadata": {},
   "source": [
    "[Solution](https://classroom.udacity.com/nanodegrees/nd320/parts/cd0568/modules/21f311a8-fcd1-488e-a66d-e460aec51b2d/lessons/97eaabd7-c746-41ce-b1e8-78b7778c04f8/concepts/b50b658d-e2f3-4ea7-a18d-703adbb345ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b85d9-63e1-43b3-9c58-07a420539ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
