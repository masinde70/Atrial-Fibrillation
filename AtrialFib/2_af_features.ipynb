{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atrial Fibrillation Features\n",
    "\n",
    "As with activity classification, we will featurize our time series signal and throw those features into a classifier. For AF we will build a two-class classifier using the inter-beat-interval time series. This time series can be derived by taking the difference between successive QRS complex locations as provided by the Pan-Tompkins algorithm in the previous videos. We source our features from a couple submissions to the [Computing in Cardiology Challenge 2017](https://physionet.org/content/challenge-2017/1.0.0/). \n",
    "\n",
    ">[Behar, Rosenberg, Yaniv, Oster. Rhythm and Quality Classification from Short ECGs Recorded Using a Mobile Device. Computing in Cardiology Challenge 2017.](http://www.cinc.org/archives/2017/pdf/165-056.pdf)  \n",
    "\n",
    "and   \n",
    ">[Bonizzi, Driessens, Karel. Detection of Atrial Fibrillation Episodes from Short Single Lead Recordings by Means of Ensemble Learning. Computing in Cardiology Challenge 2017.](http://www.cinc.org/archives/2017/pdf/169-313.pdf)\n",
    "\n",
    "\n",
    "Recall that the RR interval time series is irregularly sampled because we get a new measurement at each heart beat, instead of at some fixed interval in time. Many of the features we want can be computed on an irregular time series, but some, especially frequency domain features, cannot. In that case, we will first have to make this uniform by using interpolation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 300\n",
    "data_dir = 'cinc/'\n",
    "ref = pd.read_csv(data_dir + 'REFERENCE.csv')\n",
    "ref = dict(zip(ref.record, ref.rhythm))\n",
    "base = lambda f: os.path.splitext(os.path.basename(f))[0]\n",
    "files = sorted(glob.glob(data_dir + '*.npz'))\n",
    "qrs = []\n",
    "labels = []\n",
    "for f in files:\n",
    "    with np.load(f) as npz:\n",
    "        qrs.append(npz['qrs'])\n",
    "    labels.append(ref[base(f)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "The features for the AF detection algorithm are computed from the RR interval time series. We use the time domain and frequency domain features listed below.\n",
    "\n",
    "### Time domain\n",
    " - minimum RR interval\n",
    " - maximum RR interval\n",
    " - median RR interval\n",
    " - average RR interval\n",
    " - standard deviation of RR intervals\n",
    " - number of RR interval outliers\n",
    "   - An RR interval is an outlier if it is greater than 1.2x the average RR interval in the ECG record\n",
    "\n",
    " - root-mean-square of the difference between adjacent RR intervals\n",
    " - percent of RR interval differences greater than 50 milliseconds\n",
    "\n",
    "### Frequency domain\n",
    "The RR interval time series is not sampled regularly in time. We only have a datapoint every heart beat. Before we can compute any frequency domain features, the time series must be resampled so that we get uniform data points. Resample the RR interval time series to 4 Hz before computing the features below.\n",
    "\n",
    " - peak magnitude between 0.04 Hz and 0.15 Hz in the regularized RR interval time series\n",
    " - main frequency between 0.04 Hz and 0.15 Hz in the regularized RR interval time series\n",
    " - peak magnitude between 0.15 Hz and 0.4 Hz in the regularized RR interval time series\n",
    " - main frequency between 0.15 Hz and 0.4 Hz in the regularized RR interval time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featurize(qrs_inds, fs):\n",
    "    \"\"\"Featurize the qrs complex locations time series.\n",
    "\n",
    "    Args:\n",
    "        qrs_inds: (np.array of number) the sample indices of the QRS complex locations\n",
    "        fs: (number) the sampling rate\n",
    "\n",
    "    Returns:\n",
    "        n-tuple of features\n",
    "    \"\"\"\n",
    "    # Compute the RR interval time series\n",
    "    rr = np.diff(qrs_inds)\n",
    "\n",
    "    # Compute time domain features\n",
    "    if len(qrs_inds) < 1:\n",
    "        return [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    else:\n",
    "        pass\n",
    "    min_rr = np.min(rr)\n",
    "    max_rr = np.max(rr)\n",
    "    median_rr = np.median(rr)\n",
    "    mean_rr = np.mean(rr)\n",
    "    std_rr = np.std(rr)\n",
    "    n_outliers = np.sum(rr > 1.2 * np.mean(rr))\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(rr))))\n",
    "    pdrr_50 = np.mean(np.diff(rr) / fs > 0.05)\n",
    "\n",
    "    # Regularly resample the RR interval time series\n",
    "    rr_ts = np.arange(qrs_inds[0] / fs, qrs_inds[-2] / fs, 1 / 4)\n",
    "    rr_interp = np.interp(rr_ts, qrs_inds[:-1] / fs, rr)\n",
    "\n",
    "    # Compute the Fourier transform of the regular RR interval time series\n",
    "    freq = np.fft.rfftfreq(len(rr_interp), 1 / 4)\n",
    "    fft_mag = np.abs(np.fft.rfft(rr_interp))\n",
    "\n",
    "    # Compute frequency domain features\n",
    "    lf_mag = np.max(fft_mag[(freq >= 0.04) & (freq <= 0.15)])\n",
    "    lf_freq = freq[np.argmax(fft_mag[(freq >= 0.04) & (freq <= 0.15)])]\n",
    "    hf_mag = np.max(fft_mag[(freq >= 0.15) & (freq <= 0.4)])\n",
    "    hf_freq = freq[np.argmax(fft_mag[(freq >= 0.15) & (freq <= 0.4)])]\n",
    "\n",
    "\n",
    "    return (min_rr, max_rr, median_rr, n_outliers, mean_rr, std_rr, rmssd, pdrr_50,\n",
    "            lf_mag, lf_freq, hf_mag, hf_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [Featurize(qrs_inds, fs) for qrs_inds in qrs]      \n",
    "X, y = np.array(feats), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Write code that builds and trains a classifier to classify our ECG records.\n",
    "# Again, a random forest with 100 trees and a depth of 4 works well.\n",
    "# Evaluate the performance of the classifier using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use A RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=4,\n",
    "                             random_state=0,\n",
    "                             class_weight='balanced')\n",
    "\n",
    "# Evaluate with 7-fold cross validation\n",
    "cv = KFold(7, shuffle=True, random_state=0)\n",
    "\n",
    "# Keep an aggregated confusion matrix\n",
    "cm = np.zeros((2,2), dtype='int')\n",
    "\n",
    "# As well as aggregated predictions with labels\n",
    "y_tests = []\n",
    "y_preds = []\n",
    "\n",
    "for train_ind, test_ind in cv.split(X, y):\n",
    "    # Subset dataset\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_test, y_test = X[test_ind], y[test_ind]\n",
    "    \n",
    "    # Train model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Run model\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Aggregate results\n",
    "    y_tests.append(y_test)\n",
    "    y_preds.append(y_pred)\n",
    "    cm += confusion_matrix(y_test, y_pred, labels=['O', 'N'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Performance Metrics\n",
    "We will evaluate the classfier performance by computing precision and recall on detecting no-sinus(other) rhythm\n",
    "\n",
    "\n",
    "## Using aggregated predictions and labels\n",
    "\n",
    "From a vector of all predictions and the correct labels, we can use numpy operations to count the \n",
    "number of true positives, false positives, and false negatives. We can then use this to compute and recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7262569832402235, 0.8105299618981642)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt = np.hstack(y_tests)\n",
    "yp = np.hstack(y_preds)\n",
    "tp = np.sum((yp == 'O') & (yt == 'O'))\n",
    "fp = np.sum((yp == 'O') & (yt == 'N'))\n",
    "fn = np.sum((yp == 'N') & (yt == 'O'))\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the confusion matrix\n",
    "\n",
    "We can also get the number of true positves, false positives, and false negatives directly from the confusion matrix to compute precison and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7262569832402235, 0.8105299618981642)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = cm[0, 0]\n",
    "fp =  cm[1, 0]\n",
    "fn = cm[0, 1]\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "I used a RandomForestClassifier with 100 trees and a max depth of 4. With these basic features, it achieved a precision of 73% and a recall of 81%. Further improvements can be made by using a more advanced technique for RR interval time series regularization, and by using more informative features. See the further resources sections for example of more advanced atrial fibrillation detection algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
